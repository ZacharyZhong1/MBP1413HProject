# -*- coding: utf-8 -*-
"""Copy of Copy of MBP1413HProject.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13xC0jesfD0K0FKWZnX5p0yFKJg9HpuXM

For course project of MBP 1413H -- Denoising of Low-Field MRI Data

Written by: Xiaole Zhong

Last updated: 28 Feb 2023

Algorithm: U-Net

Dataset: IXI dataset (T1 and T2)

# Env setting
"""

# Commented out IPython magic to ensure Python compatibility.
!python -c "import monai" || pip install -q "monai-weekly[nibabel, tqdm]"
!python -c "import matplotlib" || pip install -q matplotlib
!python -c "import image_similarity_measures" || pip install image-similarity-measures
# %matplotlib inline

"""# Import package"""

import logging
import os
import shutil
import sys
import time
import random
import numpy as np
from tqdm import trange
import matplotlib.pyplot as plt
import torch
from skimage.util import random_noise
import nibabel as nib

from image_similarity_measures.quality_metrics import ssim, uiq, psnr

from monai.apps import download_and_extract
from monai.config import print_config
from monai.data import CacheDataset, DataLoader
from monai.networks.nets import UNet
from monai.transforms import (
    EnsureChannelFirstD,
    Compose,
    LoadImageD,
    RandFlipD,
    RandRotateD,
    RandZoomD,
    ScaleIntensityD,
    EnsureTypeD,
    Lambda,
    ResizeWithPadOrCropD
)
from monai.utils import set_determinism
print_config()

"""# Device"""

logging.basicConfig(stream=sys.stdout, level=logging.INFO)
set_determinism(0)
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print(device)

"""# Setup data directory"""

directory = os.environ.get("MONAI_DATA_DIRECTORY")
root_dir = "/content/drive/MyDrive/MBP1413HProjectData" if directory is None else directory
print(root_dir)

"""# Visulization function

"""

# Create small visualisation function
def plot_ims(ims, shape=None, figsize=(10, 10), titles=None):
    shape = (1, len(ims)) if shape is None else shape
    plt.subplots(*shape, figsize=figsize)
    for i, im in enumerate(ims):
        plt.subplot(*shape, i + 1)
        im = nib.load(im).get_fdata() if isinstance(im, str) else torch.squeeze(im)
        plt.imshow(im, cmap="gray")
        if titles is not None:
            plt.title(titles[i])
        plt.axis("off")
    plt.tight_layout()
    plt.show()

"""# Check data"""

Data_dir=os.path.join(root_dir, 'Train')
all_filenames = [os.path.join(Data_dir, filename) for filename in os.listdir(Data_dir)]
random.shuffle(all_filenames)

# Visualise a few of them
rand_images = np.random.choice(all_filenames, 8, replace=False)
plot_ims(rand_images, shape=(2, 4))

"""#SNR function"""



def signaltonoise(DataList):
  SNRList=[]
  reference_signal=torch.squeeze(DataList[0]).numpy()>1
  reference_signal[108:148,108:148]=True
  reference_background=torch.squeeze(DataList[0]).numpy()>1
  reference_background[10:40,10:40]=True
  for i, data in enumerate(DataList):
    data=torch.squeeze(data).numpy()
    m = np.mean(data[reference_signal])
    sd = np.std(data[reference_background])
    SNRList.append(m/sd)
  return SNRList

"""# Create the image transform chain

**Data transform chain**
"""

NoiseLambda = Lambda(
    lambda d: {
        "orig": d["im"],
        "gaus": torch.tensor(random_noise(d["im"], mode="gaussian", var=0.01), dtype=torch.float32),
    }
)

train_transforms = Compose(
    [
        LoadImageD(keys=["im"]),
        EnsureChannelFirstD(keys=["im"]),
        ScaleIntensityD(keys=["im"], minv=0.0, maxv=1.0),
        ResizeWithPadOrCropD(keys=["im"], spatial_size=[256,256]),
        RandRotateD(keys=["im"], prob=0.5, keep_size=True),
        RandFlipD(keys=["im"], spatial_axis=0, prob=0.5),
        RandZoomD(keys=["im"], min_zoom=0.9, max_zoom=1.1, prob=0.5),
        EnsureTypeD(keys=["im"]),
        NoiseLambda,
    ]
)

val_transforms = Compose(
    [
        LoadImageD(keys=["im"]),
        EnsureChannelFirstD(keys=["im"]),
        ScaleIntensityD(keys=["im"], minv=0.0, maxv=1.0),
        EnsureTypeD(keys=["im"]),
        ResizeWithPadOrCropD(keys=["im"], spatial_size=[256,256]),
        NoiseLambda,
    ]
)

"""**Create dataset and dataloader**"""

batch_size = 32
num_workers = 10

# Split into training and validation
num_val = 255
num_train = 1947
train_datadict = [{"im": fname} for fname in all_filenames[:num_train]]
val_datadict = [{"im": fname} for fname in all_filenames[-num_val:]]
print(f"total number of images: {len(all_filenames)}")
print(f"number of images for training: {len(train_datadict)}")
print(f"number of images for testing: {len(val_datadict)}")

train_ds = CacheDataset(train_datadict, train_transforms, num_workers=num_workers)
train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=num_workers)
val_ds = CacheDataset(val_datadict, val_transforms, num_workers=num_workers)
val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=num_workers)

"""**Visualization of input image**"""

# Get image original and its degraded versions
def get_single_im(ds):
    loader = torch.utils.data.DataLoader(ds, batch_size=1, num_workers=10, shuffle=True)
    itera = iter(loader)
    return next(itera)

data = get_single_im(train_ds)
plot_ims([data["orig"], data["gaus"]], titles=["orig", "Gaussian"])

SNR=signaltonoise([data["orig"], data["gaus"]])
print(f"SNR for original image: {SNR[0]}")
print(f"SNR for Noisy image: {SNR[1]}")

"""# Create Model, Loss, Optimizer"""

max_epochs = 200
val_interval = 1
VAL_AMP = True
learning_rate=1e-4

# standard PyTorch program style: create AutoEncoder, MSELoss and Adam optimizer
model = UNet(
    spatial_dims=2,
    in_channels=1,
    out_channels=1,
    act="LEAKYRELU",
    channels=(8, 16, 32, 64, 128),
    strides=(2, 2, 2, 2, 2),
    kernel_size=3,
    up_kernel_size=3,
    num_res_units=0,
    dropout=0.2,
    norm="batch"
).to(device)

loss_function = torch.nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), learning_rate)
# use amp to accelerate training
scaler = torch.cuda.amp.GradScaler()
# enable cuDNN benchmark
torch.backends.cudnn.benchmark = True

"""# Execute a typical PyTorch training process"""

best_metric = 1
best_metric_epoch = -1
best_metrics_epochs_and_time = [[], [], []]
epoch_loss_values = []
metric_values = []

total_start = time.time()
for epoch in range(max_epochs):
    epoch_start = time.time()
    print("-" * 10)
    print(f"epoch {epoch + 1}/{max_epochs}")
    model.train()
    epoch_loss = 0
    step = 0
    for batch_data in train_loader:
        step += 1
        inputs = batch_data["gaus"].to(device)
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = loss_function(outputs, batch_data["orig"].to(device))
        loss.backward()
        optimizer.step()
        epoch_loss += loss.item() * inputs.size(0)
    epoch_loss /= num_train
    epoch_loss_values.append(epoch_loss)
    print(f"epoch {epoch + 1} average loss: {epoch_loss:.4f}")

    if (epoch + 1) % val_interval == 0:
        model.eval()
        val_epoch_loss = 0
        val_step = 0
        with torch.no_grad():

            for val_data in val_loader:
                val_step += 1
                inputs = val_data["gaus"].to(device)
                outputs = model(inputs)
                loss = loss_function(outputs, val_data["orig"].to(device))
                val_epoch_loss += loss.item() * inputs.size(0)

            metric = val_epoch_loss / num_val
            metric_values.append(metric)

            if metric < best_metric:
                best_metric = metric
                best_metric_epoch = epoch + 1
                best_metrics_epochs_and_time[0].append(best_metric)
                best_metrics_epochs_and_time[1].append(best_metric_epoch)
                best_metrics_epochs_and_time[2].append(time.time() - total_start)
                torch.save(
                    model.state_dict(),
                    os.path.join(root_dir, "best_metric_model_UNet.pth"),
                )
                print("saved new best metric model")
            print(
                f"current epoch: {epoch + 1} current MSE: {metric:.4f}"
                f"\nbest mean MSE: {best_metric:.4f}"
                f" at epoch: {best_metric_epoch}"
            )
    print(f"time consuming of epoch {epoch + 1} is: {(time.time() - epoch_start):.4f}")
total_time = time.time() - total_start

print(f"train completed, best_metric: {best_metric:.4f} at epoch: {best_metric_epoch}, total time: {total_time}.")

print(model)
torch.save(
    model,
    os.path.join(root_dir, "my_model_UNet.pt"),
)

"""# Plot the mse for train and validation"""

plt.figure("train", (12, 6))
plt.subplot(1, 2, 1)
plt.title("Epoch Average Training Loss")
x = [i + 1 for i in range(len(epoch_loss_values))]
y = epoch_loss_values
plt.xlabel("epoch")
plt.ylabel("Avereged training MSE")
plt.plot(x, y, color="red")
plt.subplot(1, 2, 2)
plt.title("Epoch Average Validating Loss")
x = [val_interval * (i + 1) for i in range(len(metric_values))]
y = metric_values
plt.xlabel("epoch")
plt.ylabel("Avereged validating MSE")
plt.plot(x, y, color="green")
plt.show()

print(f"Epoch averaged MSE of training data: {min(epoch_loss_values)}")
print(f"Total MSE of validating data: {min(metric_values)}")

"""# Test data in Lock Box

Input model and data
"""

model=torch.load(os.path.join(root_dir, "my_model_UNet.pt"))
model.eval()

Test_transforms = Compose(
    [
        LoadImageD(keys=["im"]),
        EnsureChannelFirstD(keys=["im"]),
        ScaleIntensityD(keys=["im"]),
        EnsureTypeD(keys=["im"]),
        ResizeWithPadOrCropD(keys=["im"], spatial_size=[256,256]),
        NoiseLambda,
    ]
)
Test_dir=os.path.join(root_dir, 'LockBox')
all_filenames = [os.path.join(Test_dir, filename) for filename in os.listdir(Test_dir)]

Test_datadict = [{"im": fname} for fname in all_filenames[:]]
Test_ds = CacheDataset(Test_datadict, Test_transforms, num_workers=10)
Test_loader = DataLoader(Test_ds, batch_size=1, shuffle=False, num_workers=10)

"""Test output of one random sample"""

def signaltonoiseT2(DataList):
  SNRList=[]
  reference_signal=torch.squeeze(DataList[0]).numpy()>1
  reference_signal[108:148,108:148]=True
  reference_background=torch.squeeze(DataList[0]).numpy()>1
  reference_background[20:30,20:30]=True
  for i, data in enumerate(DataList):
    data=torch.squeeze(data).numpy()
    m = np.mean(data[reference_signal])
    sd = np.std(data[reference_background])
    SNRList.append(m/sd)
  return SNRList

with torch.no_grad():
  data = get_single_im(Test_ds)
  im = data["gaus"]
  recon = model(im.to(device)).detach().cpu()
  
  plot_ims(
    [data["orig"], data["gaus"]] + [recon],
    titles=["orig", "Gaussian"] + ["recon"],
  )

  SNR=signaltonoiseT2([data["orig"], data["gaus"]])
  print(f"SNR for original image: {SNR[0]}")
  print(f"SNR for noisy image: {SNR[1]}")

  SNR=signaltonoiseT2([recon])
  print(f"SNR for denoise image: {SNR[0]}")

"""Test output of one T1 sample"""

def signaltonoiseT1(DataList):
  SNRList=[]
  reference_signal=torch.squeeze(DataList[0]).numpy()>1
  reference_signal[118:138,118:138]=True
  reference_background=torch.squeeze(DataList[0]).numpy()>1
  reference_background[63:73,20:30]=True
  for i, data in enumerate(DataList):
    data=torch.squeeze(data).numpy()
    m = np.mean(data[reference_signal])
    sd = np.std(data[reference_background])
    SNRList.append(m/sd)
  return SNRList

with torch.no_grad():
  data = get_single_im(Test_ds)
  im = data["gaus"]
  recon = model(im.to(device)).detach().cpu()
  
  plot_ims(
    [data["orig"], data["gaus"]] + [recon],
    titles=["orig", "Gaussian"] + ["recon"],
  )

  SNR=signaltonoiseT1([data["orig"], data["gaus"]])
  print(f"SNR for original image: {SNR[0]}")
  print(f"SNR for noisy image: {SNR[1]}")

  SNR=signaltonoiseT1([recon])
  print(f"SNR for denoise image: {SNR[0]}")

"""Test MSE for entire test set"""

with torch.no_grad():
  test_step = 0
  test_epoch_loss = 0
  subject_mse = []
  for test_data in Test_loader:
      test_step += 1
      inputs = test_data["gaus"].to(device)
      outputs = model(inputs)
      loss = loss_function(outputs, test_data["orig"].to(device))
      subject_mse.append(loss.to('cpu'))
      test_epoch_loss += loss.item()

  metric = test_epoch_loss / test_step
  print(f"Total MSE of Lock Box data: {metric}")
  plt.title("MSE of Lock Box data")
  x = [val_interval * (i + 1) for i in range(len(subject_mse))]
  y = subject_mse
  plt.xlabel("Subjects")
  plt.ylabel("MSE")
  plt.plot(x, y, color="blue")
  plt.show()
  print(f": {subject_mse}")

"""Test MSE for T1 test set"""

model=torch.load(os.path.join(root_dir, "my_model_UNet.pt"))
model.eval()

Test_transforms = Compose(
    [
        LoadImageD(keys=["im"]),
        EnsureChannelFirstD(keys=["im"]),
        ScaleIntensityD(keys=["im"]),
        EnsureTypeD(keys=["im"]),
        ResizeWithPadOrCropD(keys=["im"], spatial_size=[256,256]),
        NoiseLambda,
    ]
)
Test_dir=os.path.join(root_dir, 'LockBoxT1')
all_filenames = [os.path.join(Test_dir, filename) for filename in os.listdir(Test_dir)]

Test_datadict = [{"im": fname} for fname in all_filenames[:len(all_filenames)]]
Test_ds = CacheDataset(Test_datadict, Test_transforms, num_workers=10)
Test_loader = DataLoader(Test_ds, batch_size=1, shuffle=False, num_workers=10)

with torch.no_grad():
  test_step = 0
  test_epoch_loss = 0
  subject_mse = []
  for test_data in Test_loader:
      test_step += 1
      inputs = test_data["gaus"].to(device)
      outputs = model(inputs)
      loss = loss_function(outputs, test_data["orig"].to(device))
      test_epoch_loss += loss.item()
      subject_mse.append(loss.to('cpu'))

  print(test_step)
  metric = test_epoch_loss / test_step
  print(f"Averaged MSE of T1 Lock Box data: {metric}")

  plt.title("MSE of T1 Lock Box data")
  x = [val_interval * (i + 1) for i in range(len(subject_mse))]
  y = subject_mse
  plt.xlabel("Subjects")
  plt.ylabel("MSE")
  plt.plot(x, y, color="green")
  plt.show()
print(f": {subject_mse}")

"""Test MSE for T2 test set"""

model=torch.load(os.path.join(root_dir, "my_model_UNet.pt"))
model.eval()

Test_transforms = Compose(
    [
        LoadImageD(keys=["im"]),
        EnsureChannelFirstD(keys=["im"]),
        ScaleIntensityD(keys=["im"]),
        EnsureTypeD(keys=["im"]),
        ResizeWithPadOrCropD(keys=["im"], spatial_size=[256,256]),
        NoiseLambda,
    ]
)
Test_dir=os.path.join(root_dir, 'LockBoxT2')
all_filenames = [os.path.join(Test_dir, filename) for filename in os.listdir(Test_dir)]

Test_datadict = [{"im": fname} for fname in all_filenames[:len(all_filenames)]]
Test_ds = CacheDataset(Test_datadict, Test_transforms, num_workers=10)
Test_loader = DataLoader(Test_ds, batch_size=1, shuffle=False, num_workers=10)

with torch.no_grad():
  test_step = 0
  test_epoch_loss = 0
  subject_mse = []
  for test_data in Test_loader:
      test_step += 1
      inputs = test_data["gaus"].to(device)
      outputs = model(inputs)
      loss = loss_function(outputs, test_data["orig"].to(device))
      test_epoch_loss += loss.item()
      subject_mse.append(loss.to('cpu'))

  metric = test_epoch_loss / test_step
  print(f"Averaged MSE of T2 Lock Box data: {metric}")
  plt.title("MSE of T2 Lock Box data")
  x = [val_interval * (i + 1) for i in range(len(subject_mse))]
  y = subject_mse
  plt.xlabel("Subjects")
  plt.ylabel("MSE")
  plt.plot(x, y, color="red")
  plt.show()
  print(f": {subject_mse}")

"""# Test fidelity

**Test SSIM for T1 image**
"""

directory = os.environ.get("MONAI_DATA_DIRECTORY")
root_dir = "/content/drive/MyDrive/MBP1413HProjectData" if directory is None else directory

device="cuda:0"

val_interval=1

model=torch.load(os.path.join(root_dir, "my_model_UNet.pt"))
model.eval()

Test_transforms = Compose(
    [
        LoadImageD(keys=["im"]),
        EnsureChannelFirstD(keys=["im"]),
        ScaleIntensityD(keys=["im"]),
        EnsureTypeD(keys=["im"]),
        ResizeWithPadOrCropD(keys=["im"], spatial_size=[256,256]),
        NoiseLambda,
    ]
)
Test_dir=os.path.join(root_dir, 'LockBoxT1')
all_filenames = [os.path.join(Test_dir, filename) for filename in os.listdir(Test_dir)]

Test_datadict = [{"im": fname} for fname in all_filenames[:len(all_filenames)]]
Test_ds = CacheDataset(Test_datadict, Test_transforms, num_workers=10)
Test_loader = DataLoader(Test_ds, batch_size=1, shuffle=False, num_workers=10)

with torch.no_grad():
  test_step = 0
  test_epoch_loss = 0
  subject_mse = []
  for test_data in Test_loader:
      test_step += 1
      inputs = test_data["gaus"].to(device)
      outputs = model(inputs)
      org_img=(torch.squeeze(test_data["orig"]).to('cpu')).numpy()
      pred_img=(torch.squeeze(outputs).to('cpu')).numpy()
      loss=ssim(org_img=org_img, pred_img=pred_img)
      subject_mse.append(loss)
      test_epoch_loss += loss

  metric = test_epoch_loss / test_step
  print(f"Averaged SSIM of T1 Lock Box data: {metric}")

  plt.title("SSIM of T1 Lock Box data")
  x = [val_interval * (i + 1) for i in range(len(subject_mse))]
  y = subject_mse
  plt.xlabel("Subjects")
  plt.ylabel("SSIM")
  plt.plot(x, y, color="green")
  plt.show()

print(f": {subject_mse}")

"""**Test UIQ for T1 image**"""

with torch.no_grad():
  test_step = 0
  test_epoch_loss = 0
  subject_mse = []
  for test_data in Test_loader:
      test_step += 1
      inputs = test_data["gaus"].to(device)
      outputs = model(inputs)
      org_img=(torch.squeeze(test_data["orig"]).to('cpu')).numpy()
      pred_img=(torch.squeeze(outputs).to('cpu')).numpy()
      org_img=org_img[:, :, np.newaxis]
      pred_img=pred_img[:, :, np.newaxis]
      loss=uiq(org_img, pred_img)
      subject_mse.append(loss)
      test_epoch_loss += loss

  metric = test_epoch_loss / test_step
  print(f"Averaged UIQ of T1 Lock Box data: {metric}")

  plt.title("UIQ of T1 Lock Box data")
  x = [val_interval * (i + 1) for i in range(len(subject_mse))]
  y = subject_mse
  plt.xlabel("Subjects")
  plt.ylabel("UIQ")
  plt.plot(x, y, color="green")
  plt.show()
print(f": {subject_mse}")

"""**Test PSNR for T1 image**"""

with torch.no_grad():
  test_step = 0
  test_epoch_loss = 0
  subject_mse = []
  for test_data in Test_loader:
      test_step += 1
      inputs = test_data["gaus"].to(device)
      outputs = model(inputs)
      org_img=(torch.squeeze(test_data["orig"]).to('cpu')).numpy()
      pred_img=(torch.squeeze(outputs).to('cpu')).numpy()
      org_img=org_img[:, :, np.newaxis]
      pred_img=pred_img[:, :, np.newaxis]
      loss=psnr(org_img, pred_img)
      subject_mse.append(loss)
      test_epoch_loss += loss

  metric = test_epoch_loss / test_step
  print(f"Averaged PSNR of T1 Lock Box data: {metric}")

  plt.title("PSNR of T1 Lock Box data")
  x = [val_interval * (i + 1) for i in range(len(subject_mse))]
  y = subject_mse
  plt.xlabel("Subjects")
  plt.ylabel("PSNR")
  plt.plot(x, y, color="green")
  plt.show()


print(f": {subject_mse}")

"""**Test SSIM for T2 image**"""

directory = os.environ.get("MONAI_DATA_DIRECTORY")
root_dir = "/content/drive/MyDrive/MBP1413HProjectData" if directory is None else directory

device="cuda:0"

val_interval=1

model=torch.load(os.path.join(root_dir, "my_model_UNet.pt"))
model.eval()

Test_transforms = Compose(
    [
        LoadImageD(keys=["im"]),
        EnsureChannelFirstD(keys=["im"]),
        ScaleIntensityD(keys=["im"]),
        EnsureTypeD(keys=["im"]),
        ResizeWithPadOrCropD(keys=["im"], spatial_size=[256,256]),
        NoiseLambda,
    ]
)
Test_dir=os.path.join(root_dir, 'LockBoxT2')
all_filenames = [os.path.join(Test_dir, filename) for filename in os.listdir(Test_dir)]

Test_datadict = [{"im": fname} for fname in all_filenames[:len(all_filenames)]]
Test_ds = CacheDataset(Test_datadict, Test_transforms, num_workers=10)
Test_loader = DataLoader(Test_ds, batch_size=1, shuffle=False, num_workers=10)

with torch.no_grad():
  test_step = 0
  test_epoch_loss = 0
  subject_mse = []
  for test_data in Test_loader:
      test_step += 1
      inputs = test_data["gaus"].to(device)
      outputs = model(inputs)
      org_img=(torch.squeeze(test_data["orig"]).to('cpu')).numpy()
      pred_img=(torch.squeeze(outputs).to('cpu')).numpy()
      loss=ssim(org_img=org_img, pred_img=pred_img)
      subject_mse.append(loss)
      test_epoch_loss += loss

  metric = test_epoch_loss / test_step
  print(f"Averaged SSIM of T2 Lock Box data: {metric}")

  plt.title("SSIM of T2 Lock Box data")
  x = [val_interval * (i + 1) for i in range(len(subject_mse))]
  y = subject_mse
  plt.xlabel("Subjects")
  plt.ylabel("SSIM")
  plt.plot(x, y, color="red")
  plt.show()


print(f": {subject_mse}")

"""**Test UIQ for T2 image**"""

with torch.no_grad():
  test_step = 0
  test_epoch_loss = 0
  subject_mse = []
  for test_data in Test_loader:
      test_step += 1
      inputs = test_data["gaus"].to(device)
      outputs = model(inputs)
      org_img=(torch.squeeze(test_data["orig"]).to('cpu')).numpy()
      pred_img=(torch.squeeze(outputs).to('cpu')).numpy()
      org_img=org_img[:, :, np.newaxis]
      pred_img=pred_img[:, :, np.newaxis]
      loss=uiq(org_img, pred_img)
      subject_mse.append(loss)
      test_epoch_loss += loss

  metric = test_epoch_loss / test_step
  print(f"Averaged UIQ of T2 Lock Box data: {metric}")

  plt.title("UIQ of T2 Lock Box data")
  x = [val_interval * (i + 1) for i in range(len(subject_mse))]
  y = subject_mse
  plt.xlabel("Subjects")
  plt.ylabel("UIQ")
  plt.plot(x, y, color="green")
  plt.show()


print(f": {subject_mse}")

"""**Test PSNR for T2 image**"""

with torch.no_grad():
  test_step = 0
  test_epoch_loss = 0
  subject_mse = []
  for test_data in Test_loader:
      test_step += 1
      inputs = test_data["gaus"].to(device)
      outputs = model(inputs)
      org_img=(torch.squeeze(test_data["orig"]).to('cpu')).numpy()
      pred_img=(torch.squeeze(outputs).to('cpu')).numpy()
      org_img=org_img[:, :, np.newaxis]
      pred_img=pred_img[:, :, np.newaxis]
      loss=psnr(org_img, pred_img)
      subject_mse.append(loss)
      test_epoch_loss += loss

  metric = test_epoch_loss / test_step
  print(f"Averaged PSNR of T2 Lock Box data: {metric}")

  plt.title("PSNR of T2 Lock Box data")
  x = [val_interval * (i + 1) for i in range(len(subject_mse))]
  y = subject_mse
  plt.xlabel("Subjects")
  plt.ylabel("PSNR")
  plt.plot(x, y, color="green")
  plt.show()


print(f": {subject_mse}")

"""# Noise Image Metrics

Test SSIM for T1 image
"""

directory = os.environ.get("MONAI_DATA_DIRECTORY")
root_dir = "/content/drive/MyDrive/MBP1413HProjectData" if directory is None else directory

device="cuda:0"

val_interval=1

model=torch.load(os.path.join(root_dir, "my_model.pt"))
model.eval()

Test_transforms = Compose(
    [
        LoadImageD(keys=["im"]),
        EnsureChannelFirstD(keys=["im"]),
        ScaleIntensityD(keys=["im"]),
        EnsureTypeD(keys=["im"]),
        ResizeWithPadOrCropD(keys=["im"], spatial_size=[256,256]),
        NoiseLambda,
    ]
)
Test_dir=os.path.join(root_dir, 'LockBoxT1')
all_filenames = [os.path.join(Test_dir, filename) for filename in os.listdir(Test_dir)]

Test_datadict = [{"im": fname} for fname in all_filenames[:len(all_filenames)]]
Test_ds = CacheDataset(Test_datadict, Test_transforms, num_workers=10)
Test_loader = DataLoader(Test_ds, batch_size=1, shuffle=False, num_workers=10)

with torch.no_grad():
  test_step = 0
  test_epoch_loss = 0
  subject_mse = []
  for test_data in Test_loader:
      test_step += 1
      org_img=(torch.squeeze(test_data["orig"]).to('cpu')).numpy()
      pred_img=(torch.squeeze(test_data["gaus"]).to('cpu')).numpy()
      loss=ssim(org_img=org_img, pred_img=pred_img)
      subject_mse.append(loss)
      test_epoch_loss += loss

  metric = test_epoch_loss / test_step
  print(f"Averaged SSIM of T1 Lock Box data: {metric}")

  plt.title("SSIM of T1 Lock Box data")
  x = [val_interval * (i + 1) for i in range(len(subject_mse))]
  y = subject_mse
  plt.xlabel("Subjects")
  plt.ylabel("SSIM")
  plt.plot(x, y, color="green")
  plt.show()

print(f": {subject_mse}")

with torch.no_grad():
  test_step = 0
  test_epoch_loss = 0
  subject_mse = []
  for test_data in Test_loader:
      test_step += 1
      inputs = test_data["gaus"].to(device)
      loss = loss_function(inputs, test_data["orig"].to(device))
      subject_mse.append(loss.to('cpu'))
      test_epoch_loss += loss.item()

  metric = test_epoch_loss / test_step
  print(f"Averaged MSE of T1 Lock Box data: {metric}")
  plt.title("MSE of T1 Lock Box data")
  x = [val_interval * (i + 1) for i in range(len(subject_mse))]
  y = subject_mse
  plt.xlabel("Subjects")
  plt.ylabel("MSE")
  plt.plot(x, y, color="green")
  plt.show()
  print(f": {subject_mse}")

  with torch.no_grad():
    test_step = 0
    test_epoch_loss = 0
    subject_mse = []
    for test_data in Test_loader:
        test_step += 1
        inputs = test_data["gaus"].to(device)
        org_img=(torch.squeeze(test_data["orig"]).to('cpu')).numpy()
        pred_img=(torch.squeeze(inputs).to('cpu')).numpy()
        org_img=org_img[:, :, np.newaxis]
        pred_img=pred_img[:, :, np.newaxis]
        loss=psnr(org_img, pred_img)
        subject_mse.append(loss)
        test_epoch_loss += loss

  metric = test_epoch_loss / test_step
  print(f"Averaged PSNR of T1 Lock Box data: {metric}")
  plt.title("PSNR of T1 Lock Box data")
  x = [val_interval * (i + 1) for i in range(len(subject_mse))]
  y = subject_mse
  plt.xlabel("Subjects")
  plt.ylabel("PSNR")
  plt.plot(x, y, color="green")
  plt.show()
  print(f": {subject_mse}")

with torch.no_grad():
    test_step = 0
    test_epoch_loss = 0
    subject_mse = []
    for test_data in Test_loader:
        test_step += 1
        inputs = test_data["gaus"].to(device)
        pred_img=(torch.squeeze(inputs).to('cpu')).numpy()
        org_img=(torch.squeeze(test_data["orig"]).to('cpu')).numpy()
        org_img=org_img[:, :, np.newaxis]
        pred_img=pred_img[:, :, np.newaxis]
        loss=uiq(org_img, pred_img)
        subject_mse.append(loss)
        test_epoch_loss += loss

metric = test_epoch_loss / test_step
print(f"Averaged PSNR of T1 Lock Box data: {metric}")
plt.title("PSNR of T1 Lock Box data")
x = [val_interval * (i + 1) for i in range(len(subject_mse))]
y = subject_mse
plt.xlabel("Subjects")
plt.ylabel("PSNR")
plt.plot(x, y, color="green")
plt.show()
print(f": {subject_mse}")

directory = os.environ.get("MONAI_DATA_DIRECTORY")
root_dir = "/content/drive/MyDrive/MBP1413HProjectData" if directory is None else directory

device="cuda:0"

val_interval=1

model=torch.load(os.path.join(root_dir, "my_model.pt"))
model.eval()

Test_transforms = Compose(
    [
        LoadImageD(keys=["im"]),
        EnsureChannelFirstD(keys=["im"]),
        ScaleIntensityD(keys=["im"]),
        EnsureTypeD(keys=["im"]),
        ResizeWithPadOrCropD(keys=["im"], spatial_size=[256,256]),
        NoiseLambda,
    ]
)
Test_dir=os.path.join(root_dir, 'LockBoxT2')
all_filenames = [os.path.join(Test_dir, filename) for filename in os.listdir(Test_dir)]

Test_datadict = [{"im": fname} for fname in all_filenames[:len(all_filenames)]]
Test_ds = CacheDataset(Test_datadict, Test_transforms, num_workers=10)
Test_loader = DataLoader(Test_ds, batch_size=1, shuffle=False, num_workers=10)

with torch.no_grad():
  test_step = 0
  test_epoch_loss = 0
  subject_mse = []
  for test_data in Test_loader:
      test_step += 1
      org_img=(torch.squeeze(test_data["orig"]).to('cpu')).numpy()
      pred_img=(torch.squeeze(test_data["gaus"]).to('cpu')).numpy()
      loss=ssim(org_img=org_img, pred_img=pred_img)
      subject_mse.append(loss)
      test_epoch_loss += loss

  metric = test_epoch_loss / test_step
  print(f"Averaged SSIM of T2 Lock Box data: {metric}")

  plt.title("SSIM of T2 Lock Box data")
  x = [val_interval * (i + 1) for i in range(len(subject_mse))]
  y = subject_mse
  plt.xlabel("Subjects")
  plt.ylabel("SSIM")
  plt.plot(x, y, color="red")
  plt.show()

print(f": {subject_mse}")

with torch.no_grad():
  test_step = 0
  test_epoch_loss = 0
  subject_mse = []
  for test_data in Test_loader:
      test_step += 1
      inputs = test_data["gaus"].to(device)
      loss = loss_function(inputs, test_data["orig"].to(device))
      subject_mse.append(loss.to('cpu'))
      test_epoch_loss += loss.item()

  metric = test_epoch_loss / test_step
  print(f"Averaged MSE of T2 Lock Box data: {metric}")
  plt.title("MSE of T2 Lock Box data")
  x = [val_interval * (i + 1) for i in range(len(subject_mse))]
  y = subject_mse
  plt.xlabel("Subjects")
  plt.ylabel("MSE")
  plt.plot(x, y, color="red")
  plt.show()
  print(f": {subject_mse}")

  with torch.no_grad():
    test_step = 0
    test_epoch_loss = 0
    subject_mse = []
    for test_data in Test_loader:
        test_step += 1
        inputs = test_data["gaus"].to(device)
        org_img=(torch.squeeze(test_data["orig"]).to('cpu')).numpy()
        pred_img=(torch.squeeze(inputs).to('cpu')).numpy()
        org_img=org_img[:, :, np.newaxis]
        pred_img=pred_img[:, :, np.newaxis]
        loss=psnr(org_img, pred_img)
        subject_mse.append(loss)
        test_epoch_loss += loss

  metric = test_epoch_loss / test_step
  print(f"Averaged PSNR of T2 Lock Box data: {metric}")
  plt.title("PSNR of T2 Lock Box data")
  x = [val_interval * (i + 1) for i in range(len(subject_mse))]
  y = subject_mse
  plt.xlabel("Subjects")
  plt.ylabel("PSNR")
  plt.plot(x, y, color="red")
  plt.show()
  print(f": {subject_mse}")

with torch.no_grad():
    test_step = 0
    test_epoch_loss = 0
    subject_mse = []
    for test_data in Test_loader:
        test_step += 1
        inputs = test_data["gaus"].to(device)
        pred_img=(torch.squeeze(inputs).to('cpu')).numpy()
        org_img=(torch.squeeze(test_data["orig"]).to('cpu')).numpy()
        org_img=org_img[:, :, np.newaxis]
        pred_img=pred_img[:, :, np.newaxis]
        loss=uiq(org_img, pred_img)
        subject_mse.append(loss)
        test_epoch_loss += loss

metric = test_epoch_loss / test_step
print(f"Averaged PSNR of Te Lock Box data: {metric}")
plt.title("UIQ of Te Lock Box data")
x = [val_interval * (i + 1) for i in range(len(subject_mse))]
y = subject_mse
plt.xlabel("Subjects")
plt.ylabel("UIQ")
plt.plot(x, y, color="red")
plt.show()
print(f": {subject_mse}")